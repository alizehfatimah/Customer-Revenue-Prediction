import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from math import sqrt
from matplotlib import pyplot as plt
from sklearn.feature_selection import mutual_info_regression as mireg
from sklearn.feature_selection import RFE

%%time
nms = pd.read_csv('Fulldatasetnomissing.csv')

%%time
nms.drop('Unnamed: 0', axis = 1)
y = np.log(nms['totals_transactionRevenue'])

X = nms.drop('totals_transactionRevenue', axis = 1)
X = X.drop('Unnamed: 0', axis = 1)

def encoder(dftemp):
    #print(dftemp.dtypes)
    col_list = list(dftemp.columns)
    #col_list.remove('index')
    for col in col_list:
        if dftemp[col].dtype != 'int64':
            dftemp[col].replace(np.nan, 'NAN', inplace = True)
            dftemp[col] = dftemp[col].astype(str)
            le = preprocessing.LabelEncoder()
            le.fit(dftemp[col])
            dftemp[col] = le.transform(dftemp[col]).reshape(-1, 1)

encoder(X)

dtr = DecisionTreeRegressor(max_depth = None)
adaboost = AdaBoostRegressor(DecisionTreeRegressor(max_depth=None), n_estimators=300, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
dtr.fit(X_train, y_train)
adaboost.fit(X_train, y_train)

%%time
y_1 = dtr.predict(X_train)

print('Score for train data and decision tree ' + str(r2_score(y_train, y_1)))
print('RMSE for train data and decision tree ' + str((mean_squared_error(y_train, y_1))**0.5))

%%time
y_2 = dtr.predict(X_test)

dtrscr = r2_score(y_test, y_2)

dtrrmse = sqrt(mean_squared_error(y_test, y_2))

print('Score for test data and decision tree ' + str(dtrscr))
print('RMSE for test data and decision tree ' + str((mean_squared_error(y_test, y_2))**0.5))

#s=pd.DataFrame(['Actual','Predicted'])
#for Actual,predicted in zip(y_test,y_2):
#    s=s.append({'Actual':Actual,'Predicted':predicted },ignore_index=True)
#s.to_csv("DecisionTreePredicted.csv")

%%time
y_3 = adaboost.predict(X_train)

print('Score for train data and adaboost ' + str(r2_score(y_train, y_3)))
print('RMSE for train data and adaboost ' + str((mean_squared_error(y_train, y_3))**0.5))

%%time
y_4 = adaboost.predict(X_test)

adascr = r2_score(y_test, y_4)

print('Score for test data and adaboost ' + str(adascr))
print('RMSE for test data and adaboost ' + str((mean_squared_error(y_test, y_4))**0.5))
#s=pd.DataFrame(['Actual','Predicted'])
#for Actual,predicted in zip(y_test,y_4):
#    s=s.append({'Actual':Actual,'Predicted':predicted },ignore_index=True)

#s.to_csv("AdaboostPredicted.csv")

%%time
regr = RandomForestRegressor(max_depth=None, random_state=0, n_estimators=100)
regr.fit(X_train, y_train)

%%time
y_5 = regr.predict(X_train)
print('Score for train data and random forest ' + str(r2_score(y_train, y_5)))
print('RMSE for train data and random forest ' + str((mean_squared_error(y_train, y_5))**0.5))

%%time
y_6 = regr.predict(X_test)

regrscr = r2_score(y_test, y_6)

regrmse = sqrt(mean_squared_error(y_test, y_6))

y_6 = pd.Series(y_6)
temp = pd.Series(y_test)
print('Score for test data and random forest ' + str(regrscr))
print('RMSE for test data and random forest ' + str((mean_squared_error(y_test, y_6))**0.5))

#s=pd.DataFrame(['Actual','Predicted'])
#for Actual,predicted in zip(temp,y_6):
#    s=s.append({'Actual':Actual,'Predicted':predicted },ignore_index=True)
    
#s.to_csv("RandomForestPredicted.csv")

%%time
#K fold cross validation
cv = KFold(n_splits=5,shuffle = True, random_state=100)
print('Cross validation for decision tree \n')
for train_index, test_index in cv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    dtr.fit(X_train, y_train)
    y_pred = dtr.predict(X_test)
    print("Root Mean Squared Error "+ str((mean_squared_error(y_test, y_pred))**0.5))
    print("\n") 
    
%%time
print('Cross validation for adaboost \n')
for train_index, test_index in cv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    adaboost.fit(X_train, y_train)
    y_pred = adaboost.predict(X_test)
    print("Root Mean Squared Error "+ str((mean_squared_error(y_test, y_pred))**0.5))
    print("\n") 
    
%%time
print('Cross validation for random forest \n')
for train_index, test_index in cv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    regr.fit(X_train, y_train)
    y_pred = regr.predict(X_test)
    print("Root Mean Squared Error "+ str((mean_squared_error(y_test, y_pred))**0.5))
    print("\n") 

scores = []
rmse = []
models = ['Decision Tree', 'Adaboost', 'Random Forest']

scores.append(dtrscr)
scores.append(adascr)
scores.append(regrscr)

dtr_ft = np.array(dtr.feature_importances_)
dtr_ft[dtr_ft>0.0005]

col = X.columns

fig, ax = plt.subplots()
plt.xticks(rotation='vertical')
rects1 = ax.bar(col, dtr.feature_importances_,color='b', error_kw={'ecolor': '0.3'})

fig, ax = plt.subplots()
plt.xticks(rotation='vertical')
rects1 = ax.bar(col, adaboost.feature_importances_,color='r', error_kw={'ecolor': '0.3'})

fig, ax = plt.subplots()
plt.xticks(rotation='vertical')
rects1 = ax.bar(col, regr.feature_importances_,color='g', error_kw={'ecolor': '0.3'})
